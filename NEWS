CHANGES IN VERSION 0.99.28
------------------------
* Deprecated `quietBCRgenes()`
* Converted `Ibex.matrix()` to `Ibex_matrix()`
* Added Install Instructions for BioCondcutor on README and Vignette
* Removed references to Keras3 Installation
* Removed LazyData TRUE statement

CHANGES IN VERSION 0.99.10
------------------------
* Added information to example data

CHANGES IN VERSION 0.99.9
------------------------
* Examples now check if python is installed and running

CHANGES IN VERSION 0.99.8
------------------------
* Updated example data to 2k HEL BEAM-Ab from 10x
* Converted ibex_example into SCE object for compliance
* Large revision of vignette to fit new data/format
* Added species argument to runIbex
* Updated CoNGA handling of assay for Seurat and Single-Cell Objects.

CHANGES IN VERSION 0.99.7
------------------------
* Integration of Ibex with immApex
* Updated Seurat object to v5
* Updated support for SCE format for ```runIbex()```
* Update ```CoNGAfy()``` to function with all versions of Seurat
* Updated ```quietBCRgenes()``` to use VariableFeatures() call for SeuratV5 and backward compatibility.
* Add ```getHumanIgPseudoGenes()``` to return a list of human Immunoglobulin Pseudo genes that are kept by ```quietBCRgenes()```

## New Models
* Added New Light and Heavy Chain Models
* Encoding methods now accepted: "OHE", "atchleyFactors", "crucianiProperties", "kideraFactors", "MSWHIM","tScales", "zScales"
* Sequence input: 
	- Human Heavy: 10000000
	- Human Light: 5000000
	- Human Heavy-Expanded: 5000000
	- Human Light-Expanded: 2500000
	- Mouse Heavy: 5000000
	- Mouse Heavy-Expanded: 5000000
* Trained convolutional and variational autoencoders for Heavy/Light chains
 	- Architecture: 512-256-128-256-512
	- Parameters: 
		Batch Size = 128
		Latent Dimensions = 128
		Epochs = 100
		Loss = Mean Squared Error (CNN) & KL Divergence (VAE)
		Activation = relu
		Learning rate = 1e-6
	- Optimizers: Adam
	- Early stopping was set to patients of 10 for minimal validation loss and restoration of best weights
	- CNN autoencoders have batch normalization layers between the dense layers. 

CHANGES IN VERSION 0.99.6
------------------------
* Implementing GitHub action workflows
* Adding testthat framework 
* Deprecating clonalCommunity 

CHANGES IN VERSION 0.99.5
------------------------
* Added geometric encoding using the BLOSUM62 matrix
* Trained classical and variational autoencoders for light/heavy chains with 1.5 million cdr sequences
 	- Architecture: 256-128-30-128-256
	- Parameters: 
		Batch Size = 64
		Latent Dimensions = 30
		Epochs = 100
		Loss = Mean Squared Error
	- Optimizers: Adam
	- Early stopping was set to patients of 10 for minimal validation loss and restoration of best weights
	- learn rate varied by models
	- classical auto encoders have batch normalization layers between the dense layers. 

CHANGES IN VERSION 0.99.4
------------------------
* Added chain.checker() function to allow for uncapitlized chain calls

CHANGES IN VERSION 0.99.3
------------------------
* Updated models for manuscript revision
 	- Architecture: 256-128-30-128-256
	- Parameters: 
		Batch Size = 64
		Learning Rate = 0.001
		Latent Dimensions = 30
		Epochs = 50
		Loss = Mean Squared Error
	- Optimizers: RAdam (for amino acid properties) and RMSprop (for OHE)
	- Early stopping was set to patients of 10 for minimal validation loss and restoration of best weights


CHANGES IN VERSION 0.99.2
------------------------
* Updated models to include radam optimization, early stop for min 10 epochs, and all trained on 800,000 unique cdr3s
* quietBCRgenes() now does not remove human Ig pseudogenes


CHANGES IN VERSION 0.99.1
------------------------
* Added detection of chain length to function call
* Added support for direct output of combineBCR()
* Modified quietBCR() to include constant regions and J-chains


CHANGES IN VERSION 0.99.0
------------------------
* Initial commit